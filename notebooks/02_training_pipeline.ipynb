{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demand Forecasting Model Training Pipeline\n",
    "\n",
    "This notebook trains demand forecasting models for each item-location combination using the feature store data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lex/miniforge3/envs/python310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import hopsworks\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Parameters\n",
    "\n",
    "Set the parameters for the training pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configure training parameters\n",
    "project_name = 'many_models'\n",
    "feature_group_name = 'demand_features'\n",
    "version = 1  # Version can be incremented automatically \n",
    "model_name = 'demand_forecaster'\n",
    "# model_version = 1  # Let Hopsworks handle versioning automatically\n",
    "test_size = 0.2\n",
    "location_id = None  # Set to specific location ID to filter for a single location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Hopsworks\n",
    "\n",
    "Establish connection to the Hopsworks Feature Store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to Hopsworks\n",
      "2025-05-09 10:21:38,202 INFO: Initializing external client\n",
      "2025-05-09 10:21:38,203 INFO: Base URL: https://demo.hops.works:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-09 10:21:39,219 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://demo.hops.works:443/p/14455\n"
     ]
    }
   ],
   "source": [
    "print(\"Connecting to Hopsworks\")\n",
    "# Connect to Hopsworks\n",
    "project = hopsworks.login(\n",
    "    host=os.getenv(\"HOST\"),\n",
    "    port=os.getenv(\"PORT\"),\n",
    "    api_key_value=os.getenv(\"HOPSWORKS_API_KEY\"),\n",
    "    project=project_name or os.getenv(\"PROJECT\")\n",
    ")\n",
    "fs = project.get_feature_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve Feature Group\n",
    "\n",
    "Get the feature group containing the demand data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving Feature Group: demand_features\n",
      "Feature Group Investigation\n",
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (0.93s) \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sp_id</th>\n",
       "      <th>loc_id</th>\n",
       "      <th>time_bucket</th>\n",
       "      <th>repetitive_demand_quantity</th>\n",
       "      <th>datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9032806</td>\n",
       "      <td>3</td>\n",
       "      <td>202404</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2025-05-09 10:05:45.219201+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8636975</td>\n",
       "      <td>3</td>\n",
       "      <td>202111</td>\n",
       "      <td>339.0</td>\n",
       "      <td>2025-05-09 10:05:45.219201+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9052071</td>\n",
       "      <td>3</td>\n",
       "      <td>202406</td>\n",
       "      <td>65.0</td>\n",
       "      <td>2025-05-09 10:05:45.219201+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9036438</td>\n",
       "      <td>3</td>\n",
       "      <td>202203</td>\n",
       "      <td>1284.0</td>\n",
       "      <td>2025-05-09 10:05:45.219201+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8413714</td>\n",
       "      <td>3</td>\n",
       "      <td>202312</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2025-05-09 10:05:45.219201+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sp_id  loc_id  time_bucket  repetitive_demand_quantity  \\\n",
       "0  9032806       3       202404                        21.0   \n",
       "1  8636975       3       202111                       339.0   \n",
       "2  9052071       3       202406                        65.0   \n",
       "3  9036438       3       202203                      1284.0   \n",
       "4  8413714       3       202312                        21.0   \n",
       "\n",
       "                          datetime  \n",
       "0 2025-05-09 10:05:45.219201+00:00  \n",
       "1 2025-05-09 10:05:45.219201+00:00  \n",
       "2 2025-05-09 10:05:45.219201+00:00  \n",
       "3 2025-05-09 10:05:45.219201+00:00  \n",
       "4 2025-05-09 10:05:45.219201+00:00  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Retrieving Feature Group: {feature_group_name}\")\n",
    "demand_fg = fs.get_feature_group(\n",
    "    name=feature_group_name,\n",
    "#    version=1,  # Using fixed version if needed\n",
    ")\n",
    "\n",
    "print(\"Feature Group Investigation\")\n",
    "demand_fg.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection and Query\n",
    "\n",
    "Select features and prepare the query for training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Selection\n"
     ]
    }
   ],
   "source": [
    "print(\"Feature Selection\")\n",
    "# Define query with proper feature selection\n",
    "query = demand_fg.select_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Transformation Functions\n",
    "\n",
    "Define transformation functions for feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up transformation functions\n",
      "Applying label encoding to location ID\n",
      "Created transformation function for loc_id using label_encoder\n"
     ]
    }
   ],
   "source": [
    "print(\"Setting up transformation functions\")\n",
    "# Import the built-in transformations\n",
    "from hopsworks.hsfs.builtin_transformations import label_encoder\n",
    "\n",
    "print(\"Applying label encoding to location ID\")\n",
    "transformation_functions = [label_encoder(\"loc_id\")]\n",
    "\n",
    "print(\"Created transformation function for loc_id using label_encoder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Feature View\n",
    "\n",
    "Create a feature view for the demand data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting or creating feature view: demand_features_view\n",
      "Feature view created successfully, explore it at \n",
      "https://demo.hops.works:443/p/14455/fs/13379/fv/demand_features_view/version/1\n",
      "Successfully got or created feature view: demand_features_view\n"
     ]
    }
   ],
   "source": [
    "print(f\"Getting or creating feature view: {feature_group_name}_view\")\n",
    "\n",
    "# Use get_or_create_feature_view method\n",
    "feature_view = fs.get_or_create_feature_view(\n",
    "    name=f\"{feature_group_name}_view\",\n",
    "    version=1,  \n",
    "    description=\"Feature view for demand forecasting\",\n",
    "    labels=[\"repetitive_demand_quantity\"],\n",
    "    query=query,\n",
    "    transformation_functions=transformation_functions\n",
    ")\n",
    "\n",
    "print(f\"Successfully got or created feature view: {feature_group_name}_view\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify Training Scope\n",
    "\n",
    "Determine the number of models to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (0.61s) \n",
      "Training 200 models (items: 200 × locations: 1)\n",
      "\n",
      "Data Overview:\n",
      "Unique items: 200\n",
      "Unique locations: 1\n"
     ]
    }
   ],
   "source": [
    "# Get model registry\n",
    "mr = project.get_model_registry()\n",
    "\n",
    "# Get unique items and locations for training\n",
    "# Read the data once and reuse it\n",
    "df = query.read()\n",
    "items = df['sp_id'].unique()  # Get unique items\n",
    "locations = df['loc_id'].unique() if location_id is None else [location_id]\n",
    "\n",
    "# Calculate total number of models\n",
    "total_models = len(items) * len(locations)\n",
    "\n",
    "print(f\"Training {total_models} models (items: {len(items)} × locations: {len(locations)})\")\n",
    "\n",
    "print(f\"\\nData Overview:\")\n",
    "print(f\"Unique items: {len(items)}\")\n",
    "print(f\"Unique locations: {len(locations)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training Loop\n",
    "\n",
    "Train models for each item-location combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model 1/200 (Item: 9032806, Location: 3)\n",
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (0.58s) \n",
      "FAILED for item 9032806, location 3: Object of type int64 is not JSON serializable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/vp/4x8yqrqn0kd9jtbp6yh_jzvr0000gn/T/ipykernel_69608/3770779722.py\", line 24, in <module>\n",
      "    X_train, X_test, y_train, y_test = feature_view.train_test_split(\n",
      "  File \"/Users/lex/miniforge3/envs/python310/lib/python3.10/site-packages/hopsworks_common/usage.py\", line 236, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/lex/miniforge3/envs/python310/lib/python3.10/site-packages/hsfs/feature_view.py\", line 2467, in train_test_split\n",
      "    td, df = self._feature_view_engine.get_training_data(\n",
      "  File \"/Users/lex/miniforge3/envs/python310/lib/python3.10/site-packages/hsfs/core/feature_view_engine.py\", line 497, in get_training_data\n",
      "    split_df = engine.get_instance().get_training_data(\n",
      "  File \"/Users/lex/miniforge3/envs/python310/lib/python3.10/site-packages/hsfs/engine/python.py\", line 896, in get_training_data\n",
      "    return self._prepare_transform_split_df(\n",
      "  File \"/Users/lex/miniforge3/envs/python310/lib/python3.10/site-packages/hsfs/engine/python.py\", line 1005, in _prepare_transform_split_df\n",
      "    transformation_function_engine.TransformationFunctionEngine.compute_and_set_feature_statistics(\n",
      "  File \"/Users/lex/miniforge3/envs/python310/lib/python3.10/site-packages/hsfs/core/transformation_function_engine.py\", line 237, in compute_and_set_feature_statistics\n",
      "    TransformationFunctionEngine.compute_transformation_fn_statistics(\n",
      "  File \"/Users/lex/miniforge3/envs/python310/lib/python3.10/site-packages/hsfs/core/transformation_function_engine.py\", line 140, in compute_transformation_fn_statistics\n",
      "    return training_dataset_obj._statistics_engine.compute_transformation_fn_statistics(\n",
      "  File \"/Users/lex/miniforge3/envs/python310/lib/python3.10/site-packages/hsfs/core/statistics_engine.py\", line 240, in compute_transformation_fn_statistics\n",
      "    stats_str = self._profile_transformation_fn_statistics(\n",
      "  File \"/Users/lex/miniforge3/envs/python310/lib/python3.10/site-packages/hsfs/core/statistics_engine.py\", line 359, in _profile_transformation_fn_statistics\n",
      "    return self._profile_unique_values(\n",
      "  File \"/Users/lex/miniforge3/envs/python310/lib/python3.10/site-packages/hsfs/core/statistics_engine.py\", line 386, in _profile_unique_values\n",
      "    return json.dumps(stats)  # the result is a JSON string\n",
      "  File \"/Users/lex/miniforge3/envs/python310/lib/python3.10/json/__init__.py\", line 231, in dumps\n",
      "    return _default_encoder.encode(obj)\n",
      "  File \"/Users/lex/miniforge3/envs/python310/lib/python3.10/json/encoder.py\", line 199, in encode\n",
      "    chunks = self.iterencode(o, _one_shot=True)\n",
      "  File \"/Users/lex/miniforge3/envs/python310/lib/python3.10/json/encoder.py\", line 257, in iterencode\n",
      "    return _iterencode(o, 0)\n",
      "  File \"/Users/lex/miniforge3/envs/python310/lib/python3.10/json/encoder.py\", line 179, in default\n",
      "    raise TypeError(f'Object of type {o.__class__.__name__} '\n",
      "TypeError: Object of type int64 is not JSON serializable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (0.48s) \n",
      "FAILED for item 8636975, location 3: Object of type int64 is not JSON serializable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/vp/4x8yqrqn0kd9jtbp6yh_jzvr0000gn/T/ipykernel_69608/3770779722.py\", line 24, in <module>\n",
      "    X_train, X_test, y_train, y_test = feature_view.train_test_split(\n",
      "  File \"/Users/lex/miniforge3/envs/python310/lib/python3.10/site-packages/hopsworks_common/usage.py\", line 236, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/lex/miniforge3/envs/python310/lib/python3.10/site-packages/hsfs/feature_view.py\", line 2467, in train_test_split\n",
      "    td, df = self._feature_view_engine.get_training_data(\n",
      "  File \"/Users/lex/miniforge3/envs/python310/lib/python3.10/site-packages/hsfs/core/feature_view_engine.py\", line 497, in get_training_data\n",
      "    split_df = engine.get_instance().get_training_data(\n",
      "  File \"/Users/lex/miniforge3/envs/python310/lib/python3.10/site-packages/hsfs/engine/python.py\", line 896, in get_training_data\n",
      "    return self._prepare_transform_split_df(\n",
      "  File \"/Users/lex/miniforge3/envs/python310/lib/python3.10/site-packages/hsfs/engine/python.py\", line 1005, in _prepare_transform_split_df\n",
      "    transformation_function_engine.TransformationFunctionEngine.compute_and_set_feature_statistics(\n",
      "  File \"/Users/lex/miniforge3/envs/python310/lib/python3.10/site-packages/hsfs/core/transformation_function_engine.py\", line 237, in compute_and_set_feature_statistics\n",
      "    TransformationFunctionEngine.compute_transformation_fn_statistics(\n",
      "  File \"/Users/lex/miniforge3/envs/python310/lib/python3.10/site-packages/hsfs/core/transformation_function_engine.py\", line 140, in compute_transformation_fn_statistics\n",
      "    return training_dataset_obj._statistics_engine.compute_transformation_fn_statistics(\n",
      "  File \"/Users/lex/miniforge3/envs/python310/lib/python3.10/site-packages/hsfs/core/statistics_engine.py\", line 240, in compute_transformation_fn_statistics\n",
      "    stats_str = self._profile_transformation_fn_statistics(\n",
      "  File \"/Users/lex/miniforge3/envs/python310/lib/python3.10/site-packages/hsfs/core/statistics_engine.py\", line 359, in _profile_transformation_fn_statistics\n",
      "    return self._profile_unique_values(\n",
      "  File \"/Users/lex/miniforge3/envs/python310/lib/python3.10/site-packages/hsfs/core/statistics_engine.py\", line 386, in _profile_unique_values\n",
      "    return json.dumps(stats)  # the result is a JSON string\n",
      "  File \"/Users/lex/miniforge3/envs/python310/lib/python3.10/json/__init__.py\", line 231, in dumps\n",
      "    return _default_encoder.encode(obj)\n",
      "  File \"/Users/lex/miniforge3/envs/python310/lib/python3.10/json/encoder.py\", line 199, in encode\n",
      "    chunks = self.iterencode(o, _one_shot=True)\n",
      "  File \"/Users/lex/miniforge3/envs/python310/lib/python3.10/json/encoder.py\", line 257, in iterencode\n",
      "    return _iterencode(o, 0)\n",
      "  File \"/Users/lex/miniforge3/envs/python310/lib/python3.10/json/encoder.py\", line 179, in default\n",
      "    raise TypeError(f'Object of type {o.__class__.__name__} '\n",
      "TypeError: Object of type int64 is not JSON serializable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (0.63s) \n",
      "FAILED for item 9052071, location 3: Object of type int64 is not JSON serializable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/vp/4x8yqrqn0kd9jtbp6yh_jzvr0000gn/T/ipykernel_69608/3770779722.py\", line 24, in <module>\n",
      "    X_train, X_test, y_train, y_test = feature_view.train_test_split(\n",
      "  File \"/Users/lex/miniforge3/envs/python310/lib/python3.10/site-packages/hopsworks_common/usage.py\", line 236, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/lex/miniforge3/envs/python310/lib/python3.10/site-packages/hsfs/feature_view.py\", line 2467, in train_test_split\n",
      "    td, df = self._feature_view_engine.get_training_data(\n",
      "  File \"/Users/lex/miniforge3/envs/python310/lib/python3.10/site-packages/hsfs/core/feature_view_engine.py\", line 497, in get_training_data\n",
      "    split_df = engine.get_instance().get_training_data(\n",
      "  File \"/Users/lex/miniforge3/envs/python310/lib/python3.10/site-packages/hsfs/engine/python.py\", line 896, in get_training_data\n",
      "    return self._prepare_transform_split_df(\n",
      "  File \"/Users/lex/miniforge3/envs/python310/lib/python3.10/site-packages/hsfs/engine/python.py\", line 1005, in _prepare_transform_split_df\n",
      "    transformation_function_engine.TransformationFunctionEngine.compute_and_set_feature_statistics(\n",
      "  File \"/Users/lex/miniforge3/envs/python310/lib/python3.10/site-packages/hsfs/core/transformation_function_engine.py\", line 237, in compute_and_set_feature_statistics\n",
      "    TransformationFunctionEngine.compute_transformation_fn_statistics(\n",
      "  File \"/Users/lex/miniforge3/envs/python310/lib/python3.10/site-packages/hsfs/core/transformation_function_engine.py\", line 140, in compute_transformation_fn_statistics\n",
      "    return training_dataset_obj._statistics_engine.compute_transformation_fn_statistics(\n",
      "  File \"/Users/lex/miniforge3/envs/python310/lib/python3.10/site-packages/hsfs/core/statistics_engine.py\", line 240, in compute_transformation_fn_statistics\n",
      "    stats_str = self._profile_transformation_fn_statistics(\n",
      "  File \"/Users/lex/miniforge3/envs/python310/lib/python3.10/site-packages/hsfs/core/statistics_engine.py\", line 359, in _profile_transformation_fn_statistics\n",
      "    return self._profile_unique_values(\n",
      "  File \"/Users/lex/miniforge3/envs/python310/lib/python3.10/site-packages/hsfs/core/statistics_engine.py\", line 386, in _profile_unique_values\n",
      "    return json.dumps(stats)  # the result is a JSON string\n",
      "  File \"/Users/lex/miniforge3/envs/python310/lib/python3.10/json/__init__.py\", line 231, in dumps\n",
      "    return _default_encoder.encode(obj)\n",
      "  File \"/Users/lex/miniforge3/envs/python310/lib/python3.10/json/encoder.py\", line 199, in encode\n",
      "    chunks = self.iterencode(o, _one_shot=True)\n",
      "  File \"/Users/lex/miniforge3/envs/python310/lib/python3.10/json/encoder.py\", line 257, in iterencode\n",
      "    return _iterencode(o, 0)\n",
      "  File \"/Users/lex/miniforge3/envs/python310/lib/python3.10/json/encoder.py\", line 179, in default\n",
      "    raise TypeError(f'Object of type {o.__class__.__name__} '\n",
      "TypeError: Object of type int64 is not JSON serializable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (0.62s) \n",
      "FAILED for item 9036438, location 3: Object of type int64 is not JSON serializable\n",
      "Training model 5/200 (Item: 8413714, Location: 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/vp/4x8yqrqn0kd9jtbp6yh_jzvr0000gn/T/ipykernel_69608/3770779722.py\", line 24, in <module>\n",
      "    X_train, X_test, y_train, y_test = feature_view.train_test_split(\n",
      "  File \"/Users/lex/miniforge3/envs/python310/lib/python3.10/site-packages/hopsworks_common/usage.py\", line 236, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/lex/miniforge3/envs/python310/lib/python3.10/site-packages/hsfs/feature_view.py\", line 2467, in train_test_split\n",
      "    td, df = self._feature_view_engine.get_training_data(\n",
      "  File \"/Users/lex/miniforge3/envs/python310/lib/python3.10/site-packages/hsfs/core/feature_view_engine.py\", line 497, in get_training_data\n",
      "    split_df = engine.get_instance().get_training_data(\n",
      "  File \"/Users/lex/miniforge3/envs/python310/lib/python3.10/site-packages/hsfs/engine/python.py\", line 896, in get_training_data\n",
      "    return self._prepare_transform_split_df(\n",
      "  File \"/Users/lex/miniforge3/envs/python310/lib/python3.10/site-packages/hsfs/engine/python.py\", line 1005, in _prepare_transform_split_df\n",
      "    transformation_function_engine.TransformationFunctionEngine.compute_and_set_feature_statistics(\n",
      "  File \"/Users/lex/miniforge3/envs/python310/lib/python3.10/site-packages/hsfs/core/transformation_function_engine.py\", line 237, in compute_and_set_feature_statistics\n",
      "    TransformationFunctionEngine.compute_transformation_fn_statistics(\n",
      "  File \"/Users/lex/miniforge3/envs/python310/lib/python3.10/site-packages/hsfs/core/transformation_function_engine.py\", line 140, in compute_transformation_fn_statistics\n",
      "    return training_dataset_obj._statistics_engine.compute_transformation_fn_statistics(\n",
      "  File \"/Users/lex/miniforge3/envs/python310/lib/python3.10/site-packages/hsfs/core/statistics_engine.py\", line 240, in compute_transformation_fn_statistics\n",
      "    stats_str = self._profile_transformation_fn_statistics(\n",
      "  File \"/Users/lex/miniforge3/envs/python310/lib/python3.10/site-packages/hsfs/core/statistics_engine.py\", line 359, in _profile_transformation_fn_statistics\n",
      "    return self._profile_unique_values(\n",
      "  File \"/Users/lex/miniforge3/envs/python310/lib/python3.10/site-packages/hsfs/core/statistics_engine.py\", line 386, in _profile_unique_values\n",
      "    return json.dumps(stats)  # the result is a JSON string\n",
      "  File \"/Users/lex/miniforge3/envs/python310/lib/python3.10/json/__init__.py\", line 231, in dumps\n",
      "    return _default_encoder.encode(obj)\n",
      "  File \"/Users/lex/miniforge3/envs/python310/lib/python3.10/json/encoder.py\", line 199, in encode\n",
      "    chunks = self.iterencode(o, _one_shot=True)\n",
      "  File \"/Users/lex/miniforge3/envs/python310/lib/python3.10/json/encoder.py\", line 257, in iterencode\n",
      "    return _iterencode(o, 0)\n",
      "  File \"/Users/lex/miniforge3/envs/python310/lib/python3.10/json/encoder.py\", line 179, in default\n",
      "    raise TypeError(f'Object of type {o.__class__.__name__} '\n",
      "TypeError: Object of type int64 is not JSON serializable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (0.48s) \n",
      "FAILED for item 8413714, location 3: Object of type int64 is not JSON serializable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/vp/4x8yqrqn0kd9jtbp6yh_jzvr0000gn/T/ipykernel_69608/3770779722.py\", line 24, in <module>\n",
      "    X_train, X_test, y_train, y_test = feature_view.train_test_split(\n",
      "  File \"/Users/lex/miniforge3/envs/python310/lib/python3.10/site-packages/hopsworks_common/usage.py\", line 236, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/lex/miniforge3/envs/python310/lib/python3.10/site-packages/hsfs/feature_view.py\", line 2467, in train_test_split\n",
      "    td, df = self._feature_view_engine.get_training_data(\n",
      "  File \"/Users/lex/miniforge3/envs/python310/lib/python3.10/site-packages/hsfs/core/feature_view_engine.py\", line 497, in get_training_data\n",
      "    split_df = engine.get_instance().get_training_data(\n",
      "  File \"/Users/lex/miniforge3/envs/python310/lib/python3.10/site-packages/hsfs/engine/python.py\", line 896, in get_training_data\n",
      "    return self._prepare_transform_split_df(\n",
      "  File \"/Users/lex/miniforge3/envs/python310/lib/python3.10/site-packages/hsfs/engine/python.py\", line 1005, in _prepare_transform_split_df\n",
      "    transformation_function_engine.TransformationFunctionEngine.compute_and_set_feature_statistics(\n",
      "  File \"/Users/lex/miniforge3/envs/python310/lib/python3.10/site-packages/hsfs/core/transformation_function_engine.py\", line 237, in compute_and_set_feature_statistics\n",
      "    TransformationFunctionEngine.compute_transformation_fn_statistics(\n",
      "  File \"/Users/lex/miniforge3/envs/python310/lib/python3.10/site-packages/hsfs/core/transformation_function_engine.py\", line 140, in compute_transformation_fn_statistics\n",
      "    return training_dataset_obj._statistics_engine.compute_transformation_fn_statistics(\n",
      "  File \"/Users/lex/miniforge3/envs/python310/lib/python3.10/site-packages/hsfs/core/statistics_engine.py\", line 240, in compute_transformation_fn_statistics\n",
      "    stats_str = self._profile_transformation_fn_statistics(\n",
      "  File \"/Users/lex/miniforge3/envs/python310/lib/python3.10/site-packages/hsfs/core/statistics_engine.py\", line 359, in _profile_transformation_fn_statistics\n",
      "    return self._profile_unique_values(\n",
      "  File \"/Users/lex/miniforge3/envs/python310/lib/python3.10/site-packages/hsfs/core/statistics_engine.py\", line 386, in _profile_unique_values\n",
      "    return json.dumps(stats)  # the result is a JSON string\n",
      "  File \"/Users/lex/miniforge3/envs/python310/lib/python3.10/json/__init__.py\", line 231, in dumps\n",
      "    return _default_encoder.encode(obj)\n",
      "  File \"/Users/lex/miniforge3/envs/python310/lib/python3.10/json/encoder.py\", line 199, in encode\n",
      "    chunks = self.iterencode(o, _one_shot=True)\n",
      "  File \"/Users/lex/miniforge3/envs/python310/lib/python3.10/json/encoder.py\", line 257, in iterencode\n",
      "    return _iterencode(o, 0)\n",
      "  File \"/Users/lex/miniforge3/envs/python310/lib/python3.10/json/encoder.py\", line 179, in default\n",
      "    raise TypeError(f'Object of type {o.__class__.__name__} '\n",
      "TypeError: Object of type int64 is not JSON serializable\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m filter_cond \u001b[38;5;241m=\u001b[39m (demand_fg\u001b[38;5;241m.\u001b[39msp_id \u001b[38;5;241m==\u001b[39m item) \u001b[38;5;241m&\u001b[39m (demand_fg\u001b[38;5;241m.\u001b[39mloc_id \u001b[38;5;241m==\u001b[39m loc)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Apply train_test_split with extra_filter\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mfeature_view\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextra_filter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilter_cond\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstatistics_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Skip if we don't have enough data for this combination\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(X_train) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(X_test) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m5\u001b[39m:\n",
      "File \u001b[0;32m~/miniforge3/envs/python310/lib/python3.10/site-packages/hopsworks_common/usage.py:236\u001b[0m, in \u001b[0;36mmethod_logger.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;66;03m# Disable usage AFTER import hsfs, return function itself\u001b[39;00m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_enabled:\n\u001b[0;32m--> 236\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[1;32m    239\u001b[0m     exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/python310/lib/python3.10/site-packages/hsfs/feature_view.py:2467\u001b[0m, in \u001b[0;36mFeatureView.train_test_split\u001b[0;34m(self, test_size, train_start, train_end, test_start, test_end, description, extra_filter, statistics_config, read_options, spine, primary_key, event_time, training_helper_columns, dataframe_type, transformation_context, **kwargs)\u001b[0m\n\u001b[1;32m   2445\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_train_test_split(\n\u001b[1;32m   2446\u001b[0m     test_size\u001b[38;5;241m=\u001b[39mtest_size, train_end\u001b[38;5;241m=\u001b[39mtrain_end, test_start\u001b[38;5;241m=\u001b[39mtest_start\n\u001b[1;32m   2447\u001b[0m )\n\u001b[1;32m   2448\u001b[0m td \u001b[38;5;241m=\u001b[39m training_dataset\u001b[38;5;241m.\u001b[39mTrainingDataset(\n\u001b[1;32m   2449\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m   2450\u001b[0m     version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2465\u001b[0m     extra_filter\u001b[38;5;241m=\u001b[39mextra_filter,\n\u001b[1;32m   2466\u001b[0m )\n\u001b[0;32m-> 2467\u001b[0m td, df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_feature_view_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_training_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2468\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2469\u001b[0m \u001b[43m    \u001b[49m\u001b[43mread_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2470\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_dataset_obj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2471\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mTrainingDatasetSplit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTRAIN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTrainingDatasetSplit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTEST\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2472\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2473\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprimary_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprimary_keys\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mprimary_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2474\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevent_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevent_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2475\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_helper_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_helper_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2476\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataframe_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataframe_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2477\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransformation_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransformation_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2478\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2479\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   2480\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented version to `\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(td\u001b[38;5;241m.\u001b[39mversion),\n\u001b[1;32m   2481\u001b[0m     util\u001b[38;5;241m.\u001b[39mVersionWarning,\n\u001b[1;32m   2482\u001b[0m     stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   2483\u001b[0m )\n\u001b[1;32m   2484\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_last_accessed_training_dataset(td\u001b[38;5;241m.\u001b[39mversion)\n",
      "File \u001b[0;32m~/miniforge3/envs/python310/lib/python3.10/site-packages/hsfs/core/feature_view_engine.py:485\u001b[0m, in \u001b[0;36mFeatureViewEngine.get_training_data\u001b[0;34m(self, feature_view_obj, read_options, splits, training_dataset_obj, training_dataset_version, spine, primary_keys, event_time, training_helper_columns, dataframe_type, transformation_context)\u001b[0m\n\u001b[1;32m    483\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    484\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_feature_group_accessibility(feature_view_obj)\n\u001b[0;32m--> 485\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_batch_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_view_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining_dataset_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtd_updated\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mversion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtd_updated\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevent_start_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mend_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtd_updated\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevent_end_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwith_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43minference_helper_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprimary_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprimary_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mevent_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevent_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining_helper_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_helper_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mspine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    497\u001b[0m     split_df \u001b[38;5;241m=\u001b[39m engine\u001b[38;5;241m.\u001b[39mget_instance()\u001b[38;5;241m.\u001b[39mget_training_data(\n\u001b[1;32m    498\u001b[0m         td_updated,\n\u001b[1;32m    499\u001b[0m         feature_view_obj,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    504\u001b[0m         transformation_context\u001b[38;5;241m=\u001b[39mtransformation_context,\n\u001b[1;32m    505\u001b[0m     )\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_training_dataset_statistics(\n\u001b[1;32m    507\u001b[0m         feature_view_obj, td_updated, split_df\n\u001b[1;32m    508\u001b[0m     )\n",
      "File \u001b[0;32m~/miniforge3/envs/python310/lib/python3.10/site-packages/hsfs/core/feature_view_engine.py:315\u001b[0m, in \u001b[0;36mFeatureViewEngine.get_batch_query\u001b[0;34m(self, feature_view_obj, start_time, end_time, with_label, primary_keys, event_time, inference_helper_columns, training_helper_columns, training_dataset_version, spine)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_batch_query\u001b[39m(\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    303\u001b[0m     feature_view_obj,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    312\u001b[0m     spine\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    313\u001b[0m ):\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 315\u001b[0m         query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_feature_view_api\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_batch_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfeature_view_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfeature_view_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mversion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m            \u001b[49m\u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_event_time_to_timestamp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m            \u001b[49m\u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_event_time_to_timestamp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mend_time\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtraining_dataset_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_dataset_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m            \u001b[49m\u001b[43mis_python_engine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpython\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwith_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprimary_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprimary_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m            \u001b[49m\u001b[43mevent_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevent_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m            \u001b[49m\u001b[43minference_helper_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minference_helper_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtraining_helper_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_helper_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;66;03m# verify whatever is passed 1. spine group with dataframe contained, or 2. dataframe\u001b[39;00m\n\u001b[1;32m    329\u001b[0m         \u001b[38;5;66;03m# the schema has to be consistent\u001b[39;00m\n\u001b[1;32m    330\u001b[0m \n\u001b[1;32m    331\u001b[0m         \u001b[38;5;66;03m# allow passing new spine group or dataframe\u001b[39;00m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(spine, feature_group\u001b[38;5;241m.\u001b[39mSpineGroup):\n\u001b[1;32m    333\u001b[0m             \u001b[38;5;66;03m# schema of original fg on left side needs to be consistent with schema contained in the\u001b[39;00m\n\u001b[1;32m    334\u001b[0m             \u001b[38;5;66;03m# spine group to overwrite the feature group\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/python310/lib/python3.10/site-packages/hsfs/core/feature_view_api.py:206\u001b[0m, in \u001b[0;36mFeatureViewApi.get_batch_query\u001b[0;34m(self, name, version, start_time, end_time, training_dataset_version, with_label, primary_keys, event_time, inference_helper_columns, training_helper_columns, is_python_engine)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_batch_query\u001b[39m(\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    186\u001b[0m     name: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    196\u001b[0m     is_python_engine: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    197\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery.Query\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    198\u001b[0m     path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_base_path \u001b[38;5;241m+\u001b[39m [\n\u001b[1;32m    199\u001b[0m         name,\n\u001b[1;32m    200\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_VERSION,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_BATCH,\n\u001b[1;32m    204\u001b[0m     ]\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m query\u001b[38;5;241m.\u001b[39mQuery\u001b[38;5;241m.\u001b[39mfrom_response_json(\n\u001b[0;32m--> 206\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_GET\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstart_time\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mend_time\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwith_label\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mwith_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwith_primary_keys\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprimary_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwith_event_time\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minference_helper_columns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minference_helper_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtraining_helper_columns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_helper_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mis_hive_engine\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_python_engine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtd_version\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_dataset_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m     )\n",
      "File \u001b[0;32m~/miniforge3/envs/python310/lib/python3.10/site-packages/hopsworks_common/decorators.py:48\u001b[0m, in \u001b[0;36mconnected.<locals>.if_connected\u001b[0;34m(inst, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m inst\u001b[38;5;241m.\u001b[39m_connected:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NoHopsworksConnectionError\n\u001b[0;32m---> 48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/python310/lib/python3.10/site-packages/hopsworks_common/client/base.py:177\u001b[0m, in \u001b[0;36mClient._send_request\u001b[0;34m(self, method, path_params, query_params, headers, data, stream, files, with_base_path_params)\u001b[0m\n\u001b[1;32m    174\u001b[0m _logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl:\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m hostname_verification:\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(url, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_verify))\n\u001b[1;32m    176\u001b[0m prepped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_session\u001b[38;5;241m.\u001b[39mprepare_request(request)\n\u001b[0;32m--> 177\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprepped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_verify\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m401\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mREST_ENDPOINT \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;66;03m# refresh token and retry request - only on hopsworks\u001b[39;00m\n\u001b[1;32m    181\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_token_expired(\n\u001b[1;32m    182\u001b[0m         request, stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mTOKEN_EXPIRED_RETRY_INTERVAL, \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    183\u001b[0m     )\n",
      "File \u001b[0;32m~/miniforge3/envs/python310/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/miniforge3/envs/python310/lib/python3.10/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/miniforge3/envs/python310/lib/python3.10/site-packages/urllib3/connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    784\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    803\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/python310/lib/python3.10/site-packages/urllib3/connectionpool.py:534\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 534\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    536\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m~/miniforge3/envs/python310/lib/python3.10/site-packages/urllib3/connection.py:516\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    513\u001b[0m _shutdown \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshutdown\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    515\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 516\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    519\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[0;32m~/miniforge3/envs/python310/lib/python3.10/http/client.py:1375\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1374\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1375\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1376\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1377\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/miniforge3/envs/python310/lib/python3.10/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/python310/lib/python3.10/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/python310/lib/python3.10/socket.py:717\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    716\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 717\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    718\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    719\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/python310/lib/python3.10/ssl.py:1307\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1303\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1304\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1305\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1306\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/miniforge3/envs/python310/lib/python3.10/ssl.py:1163\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1161\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1162\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1163\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1164\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1165\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Dictionary to store metrics for all models\n",
    "all_model_metrics = {}\n",
    "\n",
    "# Counter for progress tracking\n",
    "model_counter = 0\n",
    "\n",
    "# Loop through each item-location combination\n",
    "for item in items:\n",
    "    for loc in locations:\n",
    "        model_counter += 1\n",
    "        \n",
    "        # Display progress periodically\n",
    "        if model_counter % 5 == 0 or model_counter == 1:\n",
    "            print(f\"Training model {model_counter}/{total_models} (Item: {item}, Location: {loc})\")\n",
    "        \n",
    "        try:\n",
    "            # Create a filter for this item-location combination\n",
    "            from hsfs.constructor.filter import Filter\n",
    "            \n",
    "            # Create filter using feature group \n",
    "            filter_cond = (demand_fg.sp_id == item) and (demand_fg.loc_id == loc)\n",
    "            \n",
    "            # Apply train_test_split with extra_filter\n",
    "            X_train, X_test, y_train, y_test = feature_view.train_test_split(\n",
    "                test_size=test_size,\n",
    "                extra_filter=filter_cond,\n",
    "                statistics_config=False            )\n",
    "                        \n",
    "            # Skip if we don't have enough data for this combination\n",
    "            if len(X_train) < 10 or len(X_test) < 5:\n",
    "                continue\n",
    "            \n",
    "            # Remove ID columns\n",
    "            X_train = X_train.drop(['sp_id', 'loc_id', 'datetime'], axis=1, errors='ignore')\n",
    "            X_test = X_test.drop(['sp_id', 'loc_id', 'datetime'], axis=1, errors='ignore')\n",
    "            \n",
    "            # Model name for this item-location\n",
    "            model_prefix = f\"{model_name}_item{item}_loc{loc}\"\n",
    "            \n",
    "            # Train RandomForest\n",
    "            rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "            rf_model.fit(X_train, y_train)\n",
    "            \n",
    "            # Train XGBoost\n",
    "            xgb_model = XGBRegressor(n_estimators=100, random_state=42)\n",
    "            xgb_model.fit(X_train, y_train)\n",
    "            \n",
    "            # Evaluate models\n",
    "            models = {\n",
    "                \"RandomForest\": rf_model,\n",
    "                \"XGBoost\": xgb_model\n",
    "            }\n",
    "            \n",
    "            best_model = None\n",
    "            best_rmse = float('inf')\n",
    "            best_metrics = {}\n",
    "            \n",
    "            for model_type, model in models.items():\n",
    "                # Make predictions\n",
    "                y_pred = model.predict(X_test)\n",
    "                \n",
    "                # Calculate metrics\n",
    "                mae = mean_absolute_error(y_test, y_pred)\n",
    "                mse = mean_squared_error(y_test, y_pred)\n",
    "                rmse = np.sqrt(mse)\n",
    "                r2 = r2_score(y_test, y_pred)\n",
    "                \n",
    "                metrics = {\n",
    "                    \"mae\": mae,\n",
    "                    \"rmse\": rmse,\n",
    "                    \"r2\": r2\n",
    "                }\n",
    "                \n",
    "                if model_counter % 5 == 0 or model_counter == 1:\n",
    "                    print(f\"  {model_type}: RMSE: {rmse:.2f}\")\n",
    "                \n",
    "                # Track best model\n",
    "                if rmse < best_rmse:\n",
    "                    best_rmse = rmse\n",
    "                    best_model = model\n",
    "                    best_model_type = model_type\n",
    "                    best_metrics = metrics\n",
    "            \n",
    "            # Store metrics for this item-location combination\n",
    "            all_model_metrics[f\"item_{item}_loc_{loc}\"] = {\n",
    "                \"model_type\": best_model_type,\n",
    "                \"metrics\": best_metrics\n",
    "            }\n",
    "            \n",
    "            # Create model directory\n",
    "            model_dir = model_prefix\n",
    "            os.makedirs(model_dir, exist_ok=True)\n",
    "            \n",
    "            # Save model\n",
    "            if best_model_type == \"RandomForest\":\n",
    "                joblib.dump(best_model, os.path.join(model_dir, \"model.joblib\"))\n",
    "            else:  # XGBoost\n",
    "                best_model.save_model(os.path.join(model_dir, \"model.json\"))\n",
    "            \n",
    "            # Register model in Hopsworks\n",
    "            model_api = mr.python.create_model(\n",
    "                name=model_prefix,\n",
    "                metrics=best_metrics,\n",
    "                description=f\"Demand forecaster for item {item}, location {loc}\",\n",
    "                input_example=X_train.iloc[0].to_dict() if not X_train.empty else None,\n",
    "                feature_view=feature_view\n",
    "            )\n",
    "            \n",
    "            # Upload the model and artifacts\n",
    "            model_api.save(model_dir)\n",
    "            \n",
    "            # Clean up local model directory\n",
    "            import shutil\n",
    "            shutil.rmtree(model_dir, ignore_errors=True)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"FAILED for item {item}, location {loc}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
