{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demand Forecasting Model Training Pipeline\n",
    "\n",
    "This notebook trains demand forecasting models for each item-location combination using the feature store data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import hopsworks\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Parameters\n",
    "\n",
    "Set the parameters for the training pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configure training parameters\n",
    "project_name = 'models1000'\n",
    "feature_group_name = 'demand_features'\n",
    "# version = 1  # Version can be incremented automatically in some cases\n",
    "model_name = 'demand_forecaster'\n",
    "# model_version = 1  # Let Hopsworks handle versioning automatically\n",
    "test_size = 0.2\n",
    "location_id = None  # Set to specific location ID to filter for a single location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Hopsworks\n",
    "\n",
    "Establish connection to the Hopsworks Feature Store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Connecting to Hopsworks\")\n",
    "api_key = os.getenv(\"HOPSWORKS_API_KEY\")\n",
    "host = os.getenv(\"HOST\")\n",
    "port = os.getenv(\"PORT\")\n",
    "\n",
    "project = hopsworks.login(host=host, port=port, api_key_value=api_key, project=project_name)\n",
    "fs = project.get_feature_store()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve Feature Group\n",
    "\n",
    "Get the feature group containing the demand data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Retrieving Feature Group: {feature_group_name}\")\n",
    "demand_fg = fs.get_feature_group(\n",
    "    name=feature_group_name,\n",
    "#    version=1,  # Using fixed version if needed\n",
    ")\n",
    "\n",
    "print(\"Feature Group Investigation\")\n",
    "demand_fg.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection and Query\n",
    "\n",
    "Select features and prepare the query for training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Feature Selection\")\n",
    "# Define query with proper feature selection\n",
    "query = demand_fg.select_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Transformation Functions\n",
    "\n",
    "Define transformation functions for feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Setting up transformation functions\")\n",
    "# Import the built-in transformations\n",
    "from hopsworks.hsfs.builtin_transformations import label_encoder\n",
    "\n",
    "print(\"Applying label encoding to location ID\")\n",
    "transformation_functions = [label_encoder(\"loc_id\")]\n",
    "\n",
    "print(f\"Created transformation function for loc_id using label_encoder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Feature View\n",
    "\n",
    "Create a feature view for the demand data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Getting or creating feature view: {feature_group_name}_view\")\n",
    "\n",
    "# Use get_or_create_feature_view method\n",
    "feature_view = fs.get_or_create_feature_view(\n",
    "    name=f\"{feature_group_name}_view\",\n",
    "#    version=1,  \n",
    "    description=\"Feature view for demand forecasting\",\n",
    "    labels=[\"repetitive_demand_quantity\"],\n",
    "    query=query,\n",
    "    transformation_functions=transformation_functions\n",
    ")\n",
    "\n",
    "print(f\"Successfully got or created feature view: {feature_group_name}_view\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify Training Scope\n",
    "\n",
    "Determine the number of models to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model registry\n",
    "mr = project.get_model_registry()\n",
    "\n",
    "# Get unique items and locations for training\n",
    "items = query.read(limit=1000)['sp_id'].unique()  # Use a sample to get unique items\n",
    "locations = query.read(limit=1000)['loc_id'].unique() if location_id is None else [location_id]\n",
    "\n",
    "# Calculate total number of models\n",
    "total_models = len(items) * len(locations)\n",
    "\n",
    "print(f\"Training {total_models} models (items: {len(items)} × locations: {len(locations)})\")\n",
    "\n",
    "print(f\"\\nData Overview:\")\n",
    "print(f\"Unique items: {len(items)}\")\n",
    "print(f\"Unique locations: {len(locations)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training Loop\n",
    "\n",
    "Train models for each item-location combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store metrics for all models\n",
    "all_model_metrics = {}\n",
    "\n",
    "# Counter for progress tracking\n",
    "model_counter = 0\n",
    "\n",
    "# Loop through each item-location combination\n",
    "for item in items:\n",
    "    for loc in locations:\n",
    "        model_counter += 1\n",
    "        \n",
    "        # Display progress periodically\n",
    "        if model_counter % 5 == 0 or model_counter == 1:\n",
    "            print(f\"Training model {model_counter}/{total_models} (Item: {item}, Location: {loc})\")\n",
    "        \n",
    "        try:\n",
    "            # Filter feature view for this specific item-location combination\n",
    "            item_loc_fv = feature_view.filter(\n",
    "                (feature_view.get_feature('sp_id') == item) & \n",
    "                (feature_view.get_feature('loc_id') == loc)\n",
    "            )\n",
    "            \n",
    "            # Split the data for this item-location\n",
    "            X_train, X_test, y_train, y_test = item_loc_fv.train_test_split(test_size=test_size)\n",
    "            \n",
    "            # Skip if we don't have enough data for this combination\n",
    "            if len(X_train) < 10 or len(X_test) < 5:\n",
    "                print(f\"Skipping Item: {item}, Location: {loc} due to insufficient data (train: {len(X_train)}, test: {len(X_test)})\")\n",
    "                continue\n",
    "            \n",
    "            # We already filtered for specific item/location, so we can drop these ID columns\n",
    "            X_train = X_train.drop(['sp_id', 'loc_id', 'datetime'], axis=1, errors='ignore')\n",
    "            X_test = X_test.drop(['sp_id', 'loc_id', 'datetime'], axis=1, errors='ignore')\n",
    "            \n",
    "            # Model name for this item-location\n",
    "            model_prefix = f\"{model_name}_item{item}_loc{loc}\"\n",
    "            \n",
    "            # Train RandomForest\n",
    "            rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "            rf_model.fit(X_train, y_train)\n",
    "            \n",
    "            # Train XGBoost\n",
    "            xgb_model = XGBRegressor(n_estimators=100, random_state=42)\n",
    "            xgb_model.fit(X_train, y_train)\n",
    "            \n",
    "            # Evaluate models\n",
    "            models = {\n",
    "                \"RandomForest\": rf_model,\n",
    "                \"XGBoost\": xgb_model\n",
    "            }\n",
    "            \n",
    "            best_model = None\n",
    "            best_rmse = float('inf')\n",
    "            best_metrics = {}\n",
    "            \n",
    "            for model_type, model in models.items():\n",
    "                # Make predictions\n",
    "                y_pred = model.predict(X_test)\n",
    "                \n",
    "                # Calculate metrics\n",
    "                mae = mean_absolute_error(y_test, y_pred)\n",
    "                mse = mean_squared_error(y_test, y_pred)\n",
    "                rmse = np.sqrt(mse)\n",
    "                r2 = r2_score(y_test, y_pred)\n",
    "                \n",
    "                metrics = {\n",
    "                    \"mae\": mae,\n",
    "                    \"rmse\": rmse,\n",
    "                    \"r2\": r2\n",
    "                }\n",
    "                \n",
    "                if model_counter % 5 == 0 or model_counter == 1:\n",
    "                    print(f\"  {model_type} Metrics - MAE: {mae:.2f}, RMSE: {rmse:.2f}, R²: {r2:.2f}\")\n",
    "                \n",
    "                # Track best model\n",
    "                if rmse < best_rmse:\n",
    "                    best_rmse = rmse\n",
    "                    best_model = model\n",
    "                    best_model_type = model_type\n",
    "                    best_metrics = metrics\n",
    "            \n",
    "            # Store metrics for this item-location combination\n",
    "            all_model_metrics[f\"item_{item}_loc_{loc}\"] = {\n",
    "                \"model_type\": best_model_type,\n",
    "                \"metrics\": best_metrics\n",
    "            }\n",
    "            \n",
    "            # Create model directory\n",
    "            model_dir = model_prefix\n",
    "            os.makedirs(model_dir, exist_ok=True)\n",
    "            \n",
    "            # Save model\n",
    "            if best_model_type == \"RandomForest\":\n",
    "                joblib.dump(best_model, os.path.join(model_dir, \"model.joblib\"))\n",
    "            else:  # XGBoost\n",
    "                best_model.save_model(os.path.join(model_dir, \"model.json\"))\n",
    "            \n",
    "            # Register model in Hopsworks\n",
    "            model_api = mr.python.create_model(\n",
    "                name=model_prefix,\n",
    "                metrics=best_metrics,\n",
    "                description=f\"Demand forecasting model for item {item}, location {loc} using {best_model_type}\",\n",
    "                input_example=X_train.iloc[0].to_dict() if not X_train.empty else None,\n",
    "                feature_view=feature_view\n",
    "            )\n",
    "            \n",
    "            # Upload the model and artifacts\n",
    "            model_api.save(model_dir)\n",
    "            \n",
    "            if model_counter % 5 == 0 or model_counter == 1:\n",
    "                print(f\"  Saved model for item {item}, location {loc} using {best_model_type}\")\n",
    "            \n",
    "            # Clean up local model directory\n",
    "            shutil.rmtree(model_dir, ignore_errors=True)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error training model for Item: {item}, Location: {loc}: {str(e)}\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final cleanup of any remaining model directories\n",
    "try:\n",
    "    print(\"Performing final cleanup...\")\n",
    "    import shutil\n",
    "    import glob\n",
    "    \n",
    "    # Find and remove any model directories matching the pattern\n",
    "    model_dirs = glob.glob(f\"{model_name}_item*_loc*\")\n",
    "    for dir_path in model_dirs:\n",
    "        shutil.rmtree(dir_path, ignore_errors=True)\n",
    "    \n",
    "    print(f\"Removed {len(model_dirs)} leftover model directories\")\n",
    "except Exception as final_clean_error:\n",
    "    print(f\"Warning: Error during final cleanup: {str(final_clean_error)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
