import hopsworks
import joblib
import pandas as pd
import numpy as np
import argparse
import os
import json
import matplotlib.pyplot as plt
from datetime import datetime
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Ignore warnings
import warnings
warnings.filterwarnings('ignore')

def main(project_name='models1000', feature_group_name='demand_features', version=1,
         model_name='demand_forecaster', model_version=1, test_size=0.2, location_id=None):
    """
    Training pipeline to build demand forecasting models per item-location
    """
    print("üîÆ Connecting to Hopsworks")
    api_key = os.getenv("HOPSWORKS_API_KEY")
    host = os.getenv("HOST")
    port = os.getenv("PORT")
    
    if not api_key:
        print("‚ö†Ô∏è No API key found in environment variables")
        return
        
    if project_name:
        project = hopsworks.login(host=host, port=port, api_key_value=api_key, project=project_name)
    else:
        project_name = os.getenv("PROJECT")
        project = hopsworks.login(host=host, port=port, api_key_value=api_key, project=project_name)
    
    fs = project.get_feature_store()
    
    print(f"ü™Ñ Retrieving Feature Group: {feature_group_name}")
    demand_fg = fs.get_feature_group(
        name=feature_group_name,
        version=version,
    )
    
    print("üïµüèª‚Äç‚ôÇÔ∏è Feature Group Investigation")
    print(demand_fg.show(5))
    
    print("üíº Feature Selection")
    # Define query with proper feature selection
    query = demand_fg.select_all()
    
    # Apply filters if needed
    if location_id is not None:
        query = query.filter(demand_fg.loc_id == location_id)
    
    print("ü§ñ Setting up transformation functions")
    from hopsworks.hsfs.builtin_transformations import standard_scaler
    
    # Get sample data to identify categorical columns
    sample_data = query.show(5)
    # Identify categorical columns - assume string/object types are categorical
    cat_cols = sample_data.select_dtypes(include=['object']).columns.tolist()
    
    print("ü§ñ Setting up transformation functions")
    # Import the built-in transformations directly from hopsworks module 
    # This is the preferred method according to documentation
    from hopsworks.hsfs.builtin_transformations import standard_scaler, min_max_scaler, label_encoder
        
    # Define transformations for different column types
    transformation_functions = []
    
    # Get numeric columns that need scaling
    # Explicitly identify numeric columns for clarity
    numeric_cols = [col for col in sample_data.columns 
                  if col not in cat_cols + ['datetime', 'repetitive_demand_quantity']
                  and sample_data[col].dtype in [float, int, 'float64', 'int64', 'float32', 'int32']]
    
    print(f"Identified {len(numeric_cols)} numeric columns for transformation")
    
    # Apply standard_scaler to each numeric column if any exist
    if numeric_cols:
        for col in numeric_cols:
            # Apply the transformation function to specific column
            transformation_functions.append(standard_scaler(col))
        
    # Don't apply any transformations for now to make sure we can get data
    # This will help us focus on basic functionality first
    print("Skipping transformations to ensure data availability")
    transformation_functions = []
    
    print(f"Set up transformations for {len(transformation_functions)} columns total")
        
    print("‚öôÔ∏è Creating Feature View")
    
    # Try to get or create feature view using get_or_create_feature_view
    try:
        print(f"Getting or creating feature view: {feature_group_name}_view")
        
        # Log which transformations will be applied
        transformation_info = []
        for tf in transformation_functions:
            # Get the function name and parameter
            func_str = str(tf)
            transformation_info.append(func_str)
        
        print(f"Will apply the following transformations:")
        for i, tf_info in enumerate(transformation_info):
            print(f"  {i+1}. {tf_info}")
        
        # Use get_or_create_feature_view method which handles both cases properly
        # Create with minimal transformations that won't eliminate data
        feature_view = fs.get_or_create_feature_view(
            name=f"{feature_group_name}_view",
            version=version,
            description="Feature view for demand forecasting",
            labels=["repetitive_demand_quantity"],
            query=query,
            transformation_functions=transformation_functions
        )
        print(f"Successfully got or created feature view: {feature_group_name}_view")
    
    except Exception as e:
        print(f"Error with feature view: {str(e)}")
        print(f"Detailed error: {type(e).__name__}: {str(e)}")
        
        # Try to get the view directly as a fallback
        try:
            print(f"Attempting to get existing feature view as fallback...")
            feature_view = fs.get_feature_view(name=f"{feature_group_name}_view", version=version)
            print(f"Retrieved existing feature view in fallback")
        except Exception as get_error:
            print(f"Fallback also failed: {str(get_error)}")
            raise ValueError(f"Failed to get or create feature view: {str(e)}")
    
    # Verify feature view exists
    if feature_view is None:
        raise ValueError("Failed to get or create feature view")
    
    print("üèãÔ∏è Creating Training Dataset")
    try:
        # Create a training dataset with transformations using the simplified approach seen in tutorials
        print("Applying train_test_split directly from feature view...")
        X_train, X_test, y_train, y_test = feature_view.train_test_split(
            test_size=test_size
        )
        print(f"Successfully split data using feature view: train={X_train.shape}, test={X_test.shape}")
            
    except Exception as e:
        print(f"Error creating or using training dataset: {str(e)}")
        print(f"Detailed error: {type(e).__name__}: {str(e)}")
        
        # Final fallback: Read directly from query and split manually
        print("Falling back to direct query data retrieval (without transformations)...")
        data = query.read()
        print(f"Retrieved data directly from query with shape: {data.shape}")
        
        # Split data manually
        from sklearn.model_selection import train_test_split as sk_train_test_split
        
        # Separate features and target
        X = data.drop('repetitive_demand_quantity', axis=1)
        y = data['repetitive_demand_quantity']
        
        # Split data
        X_train, X_test, y_train, y_test = sk_train_test_split(X, y, test_size=test_size, random_state=42)
        print(f"Split data into train set: {X_train.shape} and test set: {X_test.shape}")
        print("WARNING: Using data without transformations!")
    
    # Get unique items and locations
    items = X_train['sp_id'].unique()
    locations = X_train['loc_id'].unique() if location_id is None else [location_id]
    
    # Calculate total number of models
    total_models = len(items) * len(locations)
    
    # Get model registry
    mr = project.get_model_registry()
    
    print(f"üß¨ Training {total_models} models (items: {len(items)} √ó locations: {len(locations)})")
    
    # Counter for progress tracking
    model_counter = 0
    
    # Dictionary to store metrics for all models
    all_model_metrics = {}
    
    # Loop through each item and location combination
    for item in items:
        for loc in locations:
            model_counter += 1
            if model_counter % 10 == 0 or model_counter == 1:
                # Convert loc to int if it's a location ID to ensure proper display
                loc_display = int(loc) if isinstance(loc, (int, float)) or (isinstance(loc, str) and loc.isdigit()) else loc
                print(f"Training model {model_counter}/{total_models} (Item: {item}, Location: {loc_display})")
            
            # Filter data for this item-location combination
            item_loc_mask_train = (X_train['sp_id'] == item) & (X_train['loc_id'] == loc)
            item_loc_mask_test = (X_test['sp_id'] == item) & (X_test['loc_id'] == loc)
            
            # Skip if we don't have enough data for this combination
            if sum(item_loc_mask_train) < 10 or sum(item_loc_mask_test) < 5:
                # Convert loc to int if it's a location ID to ensure proper display
                loc_display = int(loc) if isinstance(loc, (int, float)) or (isinstance(loc, str) and loc.isdigit()) else loc
                print(f"‚ö†Ô∏è Skipping Item: {item}, Location: {loc_display} due to insufficient data (train: {sum(item_loc_mask_train)}, test: {sum(item_loc_mask_test)})")
                continue
            
            X_train_item = X_train[item_loc_mask_train].drop(['sp_id', 'loc_id', 'datetime'], axis=1)
            y_train_item = y_train[item_loc_mask_train]
            X_test_item = X_test[item_loc_mask_test].drop(['sp_id', 'loc_id', 'datetime'], axis=1)
            y_test_item = y_test[item_loc_mask_test]
        
            # Feature engineering - convert time_bucket to year and month if available
            for df in [X_train_item, X_test_item]:
                if not df.empty:
                    # Print available columns for debugging
                    print(f"Columns in dataframe: {df.columns.tolist()}")
                    
                    # Check if time_bucket exists, if not, we'll skip this step
                    if 'time_bucket' in df.columns:
                        df['year'] = df['time_bucket'].astype(str).str[:4].astype(int)
                        df['month'] = df['time_bucket'].astype(str).str[4:].astype(int)
                        df.drop('time_bucket', axis=1, inplace=True)
                    else:
                        # If time_bucket doesn't exist, add dummy values to avoid errors
                        print("Column 'time_bucket' not found in data, adding placeholder values")
                        df['year'] = 2023  # Default year
                        df['month'] = 1    # Default month
            
            # Skip if dataframes are empty after processing
            if X_train_item.empty or X_test_item.empty:
                # Convert loc to int if it's a location ID to ensure proper display
                loc_display = int(loc) if isinstance(loc, (int, float)) or (isinstance(loc, str) and loc.isdigit()) else loc
                print(f"‚ö†Ô∏è Skipping Item: {item}, Location: {loc_display} due to empty data after processing")
                continue
            
            # Train both models for comparison
            model_prefix = f"{model_name}_item{item}_loc{loc}"
            
            try:
                # Train RandomForest
                rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
                rf_model.fit(X_train_item, y_train_item)
                
                # Train XGBoost
                xgb_model = XGBRegressor(n_estimators=100, random_state=42)
                xgb_model.fit(X_train_item, y_train_item)
                
                # Evaluate models
                models = {
                    "RandomForest": rf_model,
                    "XGBoost": xgb_model
                }
                
                best_model = None
                best_rmse = float('inf')
                best_metrics = {}
                
                for model_type, model in models.items():
                    # Make predictions
                    y_pred = model.predict(X_test_item)
                    
                    # Calculate metrics
                    mae = mean_absolute_error(y_test_item, y_pred)
                    mse = mean_squared_error(y_test_item, y_pred)
                    rmse = np.sqrt(mse)  # Calculate RMSE manually instead of using squared=False
                    r2 = r2_score(y_test_item, y_pred)
                    
                    metrics = {
                        "mae": mae,
                        "rmse": rmse,
                        "r2": r2
                    }
                    
                    if model_counter % 10 == 0 or model_counter == 1:
                        print(f"  {model_type} Metrics - MAE: {mae:.2f}, RMSE: {rmse:.2f}, R¬≤: {r2:.2f}")
                    
                    # Track best model
                    if rmse < best_rmse:
                        best_rmse = rmse
                        best_model = model
                        best_model_type = model_type
                        best_metrics = metrics
                
                # Store metrics for this item-location combination
                all_model_metrics[f"item_{item}_loc_{loc}"] = {
                    "model_type": best_model_type,
                    "metrics": best_metrics
                }
        
                # Create model directory
                model_dir = model_prefix
                images_dir = os.path.join(model_dir, "images")
                os.makedirs(images_dir, exist_ok=True)
                
                # Create feature importance plot
                plt.figure(figsize=(10, 6))
                # For both model types, we'll use the same approach for feature importance
                importances = best_model.feature_importances_
                indices = np.argsort(importances)[::-1]
                features = X_train_item.columns
                
                plt.title(f'Feature Importances for Item {item}, Location {loc}')
                plt.bar(range(X_train_item.shape[1]), importances[indices], align='center')
                plt.xticks(range(X_train_item.shape[1]), [features[i] for i in indices], rotation=90)
                plt.tight_layout()
                
                plt.savefig(os.path.join(images_dir, "feature_importance.png"), dpi=300, bbox_inches='tight')
                plt.close()
                
                # Save model
                if best_model_type == "RandomForest":
                    joblib.dump(best_model, os.path.join(model_dir, "model.joblib"))
                else:  # XGBoost
                    best_model.save_model(os.path.join(model_dir, "model.json"))
                
                # Register model in Hopsworks with automatic versioning
                model_api = mr.python.create_model(
                    name=model_prefix,
                    metrics=best_metrics,
                    # Omit version to use automatic incremental versioning
                    description=f"Demand forecasting model for item {item}, location {loc} using {best_model_type} - {datetime.now().strftime('%Y-%m-%d')}",
                    input_example=X_train_item.iloc[0].to_dict(),
                    feature_view=feature_view
                )
                
                # Upload the model and artifacts
                model_api.save(model_dir)
                
                if model_counter % 10 == 0 or model_counter == 1:
                    print(f"  ‚úÖ Saved model for item {item}, location {loc} using {best_model_type}")
                
                # Clean up local model directory after upload to save disk space
                try:
                    import shutil
                    shutil.rmtree(model_dir, ignore_errors=True)
                    if model_counter % 50 == 0:
                        print(f"  üßπ Cleaned up local model files to save disk space (model {model_counter})")
                except Exception as clean_error:
                    # Non-critical error, just log and continue
                    print(f"  ‚ö†Ô∏è Warning: Could not clean up local model directory: {str(clean_error)}")
                
            except Exception as e:
                # Convert loc to int if it's a location ID to ensure proper display
                loc_display = int(loc) if isinstance(loc, (int, float)) or (isinstance(loc, str) and loc.isdigit()) else loc
                print(f"‚ö†Ô∏è Error training model for Item: {item}, Location: {loc_display}: {str(e)}")
                continue
    
    # Save overall metrics summary
    metrics_dir = "model_metrics"
    os.makedirs(metrics_dir, exist_ok=True)
    
    # Save metrics to JSON
    with open(os.path.join(metrics_dir, "all_model_metrics.json"), 'w') as f:
        json.dump(all_model_metrics, f, indent=2)
    
    # Create summary statistics
    model_types = [data["model_type"] for data in all_model_metrics.values()]
    rf_count = model_types.count("RandomForest")
    xgb_count = model_types.count("XGBoost")
    
    # Get mean metrics across all models
    mae_values = [data["metrics"]["mae"] for data in all_model_metrics.values()]
    rmse_values = [data["metrics"]["rmse"] for data in all_model_metrics.values()]
    r2_values = [data["metrics"]["r2"] for data in all_model_metrics.values()]
    
    # Create summary report
    summary = {
        "total_models_trained": len(all_model_metrics),
        "model_type_distribution": {
            "RandomForest": rf_count,
            "XGBoost": xgb_count
        },
        "average_metrics": {
            "mae": sum(mae_values) / len(mae_values) if mae_values else 0,
            "rmse": sum(rmse_values) / len(rmse_values) if rmse_values else 0,
            "r2": sum(r2_values) / len(r2_values) if r2_values else 0
        }
    }
    
    # Save summary
    with open(os.path.join(metrics_dir, "training_summary.json"), 'w') as f:
        json.dump(summary, f, indent=2)
    
    print(f"‚úÖ Training pipeline completed successfully: {len(all_model_metrics)} models trained")
    print(f"   RandomForest models: {rf_count}, XGBoost models: {xgb_count}")
    print(f"   Average RMSE: {summary['average_metrics']['rmse']:.2f}")
    
    # Final cleanup of any remaining model directories
    try:
        print("üßπ Performing final cleanup...")
        import shutil
        import glob
        
        # Find and remove any model directories matching the pattern
        model_dirs = glob.glob(f"{model_name}_item*_loc*")
        for dir_path in model_dirs:
            shutil.rmtree(dir_path, ignore_errors=True)
        
        print(f"   Removed {len(model_dirs)} leftover model directories")
    except Exception as final_clean_error:
        print(f"‚ö†Ô∏è Warning: Error during final cleanup: {str(final_clean_error)}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Training Pipeline Parameters')
    parser.add_argument('--project', type=str, help='Hopsworks project name')
    parser.add_argument('--feature-group', type=str, default='demand_features', 
                        help='Feature group name')
    parser.add_argument('--version', type=int, default=1, 
                        help='Feature group version')
    parser.add_argument('--model-name', type=str, default='demand_forecaster',
                        help='Base name for the models')
    parser.add_argument('--model-version', type=int, default=1,
                        help='Model version')
    parser.add_argument('--test-size', type=float, default=0.2,
                        help='Proportion of data to use for testing')
    parser.add_argument('--location', type=int, default=None,
                        help='Filter training for a specific location')
    
    args = parser.parse_args()
    
    main(
        project_name=args.project,
        feature_group_name=args.feature_group,
        version=args.version,
        model_name=args.model_name,
        model_version=args.model_version,
        test_size=args.test_size,
        location_id=args.location
    )